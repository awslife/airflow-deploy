---
- name: set profile name
  set_fact:
    profile_name: "airflow-{{ airflow_version }}-python{{ python_version }}"
    airflow_home: "/opt/airflow"
    airflow_uid: "50000"
    airflow_gid: "100"
    airflow_group: "users"
    airflow_kubernetes_in_cluster: "False"
  tags:
    - always

- name: stop airflow
  systemd:
    name: "{{ item.name }}"
    state: stopped
  with_items:
    - { name: "airflow-webserver" }
    - { name: "airflow-scheduler" }
  ignore_errors: True
  tags:
    - init
    - stop
    - service

- name: delete conda environment
  shell: |
    conda env remove -y -n {{ profile_name }}
  become_user: airflow
  ignore_errors: True
  tags:
    - init
    - stop

- name: config firewall
  block:
    - firewalld:
        port: 8080/tcp
        permanent: yes
        state: enabled
    - systemd:
        name: firewalld
        state: reloaded
  tags:
    - init
    - firewall

- name: create airflow user
  user:
    name: airflow
    uid: "{{ airflow_uid }}"
    group: "{{ airflow_group }}"
    home: "{{ airflow_home }}"
    create_home: yes
    system: yes
    state: present
  tags:
    - conf
    - user

- name: unmount airflow dag on s3
  mount:
    path: "{{ airflow_home }}/dags"
    state: unmounted
  tags:
    - conf
    - mount

- name: delete old airflow directory
  file:
    path: "{{ item.path }}"
    state: absent
  with_items:
    - { path: "{{ airflow_home }}/.kube" }
    - { path: "{{ airflow_home }}/plugins" }
    - { path: "{{ airflow_home }}/logs" }
    - { path: "{{ airflow_home }}/configs" }
    - { path: "{{ airflow_home }}/dags" }
    - { path: "/etc/s3fs" }
    - { path: "/var/run/airflow" }
    - { path: "/var/log/airflow" }
    - { path: "~airflow/.conda" }
  tags:
    - conf
    - dir

- name: create airflow directory
  file:
    path: "{{ item.path }}"
    owner: airflow
    group: "{{ airflow_group }}"
    mode: "{{ item.mode }}"
    state: directory
  with_items:
    - {
      path: "{{ airflow_home }}/.kube",
      mode: "0755",
      state: "directory",
      }
    - {
      path: "{{ airflow_home }}/plugins",
      mode: "0755",
      state: "directory",
      }
    - {
      path: "{{ airflow_home }}/logs",
      mode: "0755",
      state: "directory",
      }
    - {
      path: "{{ airflow_home }}/configs",
      mode: "0755",
      state: "directory",
      }
    - {
      path: "{{ airflow_home }}/dags",
      mode: "0755",
      state: "directory",
      }
    - {
      path: "/var/run/airflow",
      mode: "0755",
      state: "directory",
      }
    - {
      path: "/var/log/airflow",
      mode: "0755",
      state: "directory",
      }
  tags:
    - conf
    - dir

- name: create s3 access and secret key for s3fuse
  lineinfile:
    path: /etc/s3fs
    line: "{{ airflow_s3_login }}:{{ airflow_s3_password }}"
    mode: "0600"
    state: present
    create: yes
  tags:
    - conf

- name: mount dag path on s3
  mount:
    path: "{{ airflow_home }}/dags"
    src: "{{ airflow_s3_bucket }}"
    boot: yes
    fstype: "fuse.s3fs"
    opts: "url={{ airflow_s3_url }},passwd_file=/etc/s3fs,uid={{ airflow_uid }},gid={{ airflow_gid }},_netdev,use_path_request_style,allow_other,mp_umask=0022,umask=0133,dbglevel=info"
    state: mounted
  tags:
    - conf

- name: create conda environment
  shell: |
    conda config --set auto_activate_base false
    conda config --set default_threads 4
    conda create -y -n {{ profile_name }} python={{ python_version }}
  environment:
    PATH: "{{ airflow_home }}/.local/bin:{{ ansible_env.PATH }}:/usr/local/bin"
  args:
    chdir: "{{ airflow_home }}"
  become_user: airflow
  register: result
  failed_when: result.rc > 1
  tags:
    - install

- name: install user extra packages
  block:
    - name: install conda user extra packages
      shell: |
        conda install -n {{ profile_name }} -y pip postgresql
      args:
        chdir: "{{ airflow_home }}"
      register: result
      failed_when: result.rc > 1
    
    - name: install pip user extra packages
      shell: |
        conda run -n {{ profile_name }} \
          pip install --quiet --no-cache-dir --no-warn-script-location docker
      args:
        chdir: "{{ airflow_home }}"
      register: result
      failed_when: result.rc > 1
  environment:
    PATH: "{{ airflow_home }}/.local/bin:{{ ansible_env.PATH }}:/usr/local/bin"
  become_user: airflow
  tags:
    - install

- name: install airflow
  shell: |
      conda run -n {{ profile_name }} \
        pip install --quiet --no-cache-dir --no-warn-script-location \
          "apache-airflow[{{ airflow_extra_packages }}]=={{ airflow_version }}" \
          --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-{{ airflow_version }}/constraints-no-providers-{{ python_version }}.txt"
  environment:
    PATH: "{{ airflow_home }}/.local/bin:{{ ansible_env.PATH }}:/usr/local/bin"
  args:
    chdir: "{{ airflow_home }}"
  become_user: airflow
  register: result
  failed_when: result.rc > 1
  tags:
    - install

- name: copy airflow config
  copy:
    src: airflow/{{ airflow_version_list[:2] | join('.') }}/airflow.cfg
    dest: "{{ airflow_home }}/airflow.cfg"
    owner: airflow
    group: "{{ airflow_group }}"
  vars:
    airflow_version_list: "{{ airflow_version.split('.') }}"
  tags:
    - config

- name: config airflow config
  replace:
    path: "{{ airflow_home }}/airflow.cfg"
    regexp: "{{ item.regexp }}"
    replace: "{{ item.replace }}"
  with_items:
    - { regexp: "^dags_folder =.*$", replace: "dags_folder = {{ airflow_home }}/dags/s3" }
    - { regexp: "^hostname_callable =.*$", replace: "hostname_callable = airflow.utils.net.get_host_ip_address" }
    - { regexp: "^default_timezone =.*$", replace: "default_timezone = {{ airflow_core_default_timezone }}" }
    - { regexp: "^executor =.*$", replace: "executor = {{ airflow_core_executor }}" }
    - { regexp: "^sql_alchemy_conn =.*$", replace: "sql_alchemy_conn = {{ airflow_database }}" }
    - { regexp: "^load_examples =.*$", replace: "load_examples = {{ airflow_core_load_examples }}" }
    - { regexp: "^fernet_key =.*$", replace: "fernet_key = {{ airflow_fernet_key }}"}
    - { regexp: "^plugins_folder =.*$", replace: "plugins_folder = {{ airflow_home }}/plugins" }
    - { regexp: "^base_log_folder =.*$", replace: "base_log_folder = {{ airflow_home }}/logs" }
    - { regexp: "^remote_logging =.*$", replace: "remote_logging = {{ airflow_logging_remote_logging }}" }
    - { regexp: "^remote_log_conn_id =.*$", replace: "remote_log_conn_id = {{ airflow_logging_remote_log_conn_id }}" }
    - { regexp: "^remote_base_log_folder =.*$", replace: "remote_base_log_folder = {{ airflow_logging_remote_base_log_folder }}" }
    - { regexp: "^encrypt_s3_logs =.*$", replace: "encrypt_s3_logs = {{ airflow_logging_encrypt_s3_logs }}" }
    - { regexp: "^dag_processor_manager_log_location =.*$", replace: "dag_processor_manager_log_location = {{ airflow_home }}/logs/dag_processor_manager/dag_processor_manager.log" }
    - { regexp: "^endpoint_url =.*$", replace: "endpoint_url = {{ airflow_url }}" }
    - { regexp: "^base_url =.*$", replace: "base_url = {{ airflow_url }}" }
    - { regexp: "^default_ui_timezone =.*$", replace: "default_ui_timezone = {{ airflow_core_default_timezone }}" }
    - { regexp: "^result_backend =.*$", replace: "result_backend = {{ airflow_celery }}" }
    - { regexp: "^child_process_log_directory =.*$", replace: "child_process_log_directory = {{ airflow_home }}/logs/scheduler" }
    - { regexp: "^pod_template_file =.*$", replace: "pod_template_file = {{airflow_home}}/configs/pod_template.yaml" }
    - { regexp: "^worker_container_repository =.*$", replace: "worker_container_repository = {{ airflow_kubernetes_worker_container_repository }}" }
    - { regexp: "^worker_container_tag =.*$", replace: "worker_container_tag = {{ airflow_version }}-python{{ python_version }}" }
    - { regexp: "^namespace =.*$", replace: "namespace = {{ airflow_kubernetes_namespace }}" }
    - { regexp: "^in_cluster =.*$", replace: "in_cluster = {{ airflow_kubernetes_in_cluster }}" }
    - { regexp: "^# config_file =.*$", replace: "config_file = {{ airflow_home }}/.kube/config" }
    - { regexp: "^delete_worker_pods =.*$", replace: "delete_worker_pods = {{ airflow_kubernetes_delete_worker_pods }}" }
    - { regexp: "^delete_worker_pods_on_failure =.*$", replace: "delete_worker_pods_on_failure = {{ airflow_kubernetes_delete_worker_pods_on_failure }}" }
    - { regexp: "^dag_dir_list_interval =.*$", replace: "dag_dir_list_interval = {{ airflow_schduler_dag_dir_list_interval }}" }
    - { regexp: "^auth_backends =.*$", replace: "auth_backends = user_auth" }
  tags:
    - config

- name: copy airflow config
  template:
    src: "{{ item.src }}"
    dest: "{{ item.dest }}"
    owner: airflow
    group: "{{ airflow_group }}"
    mode: 0644
  with_items:
    - { src: "config/{{ airflow_version_list[:2] | join('.') }}/webserver_config.py.j2", dest: "{{ airflow_home }}/webserver_config.py" }
    - { src: "config/{{ airflow_version_list[:2] | join('.') }}/user_auth.py.j2", dest: "{{ airflow_home }}/user_auth.py" }
  vars:
    airflow_version_list:  "{{ airflow_version.split('.') }}"
  tags:
    - config

- name: initialize airflow database
  shell: |
    conda run -n {{ profile_name }} airflow db reset --yes
    conda run -n {{ profile_name }} airflow connections add '{{ airflow_s3_id }}' \
      --conn-type 's3' \
      --conn-host '{{ airflow_s3_host }}' \
      --conn-port '{{ airflow_s3_port }}' \
      --conn-login '{{ airflow_s3_login }}' \
      --conn-password '{{ airflow_s3_password }}' \
      --conn-extra '{ "host": "{{ airflow_s3_url }}", "aws_access_key_id":"{{ airflow_s3_login }}", "aws_secret_access_key": "{{ airflow_s3_password }}" }'
  environment:
    AIRFLOW_HOME: "{{ airflow_home }}"
    PATH: "{{ airflow_home }}/.local/bin:{{ ansible_env.PATH }}:/usr/local/bin"
  args:
    chdir: "{{ airflow_home }}"
  become_user: airflow
  register: result
  failed_when: result.rc > 1
  when: airflow_list[0] == inventory_hostname
  tags:
    - config

- name: set kubeconfig for kubernetes executor
  shell: |
    kubectl config set-cluster {{ k8s_cluster_name }} \
      --server={{ k8s_cluster_server }}

    kubectl config set clusters.{{ k8s_cluster_name }}.certificate-authority-data \
      --set-raw-bytes=false \
      '{{ k8s_cluster_certificate_authority_data }}'

    kubectl config set users.{{ k8s_user_name }}.client-certificate-data \
      --set-raw-bytes=false \
      '{{ k8s_user_client_certificate_data }}'

    kubectl config set users.{{ k8s_user_name }}.client-key-data \
      --set-raw-bytes=false \
      '{{ k8s_user_client_key_data }}'

    kubectl config set-context {{ k8s_user_name }}@{{ k8s_cluster_name }} \
      --namespace={{ k8s_namespace }} \
      --cluster={{ k8s_cluster_name }} \
      --user={{ k8s_user_name }}

    kubectl config use-context {{ k8s_user_name }}@{{ k8s_cluster_name }}
  environment:
    AIRFLOW_HOME: "{{ airflow_home }}"
    KUBECONFIG: "{{ airflow_home }}/.kube/config"
    PATH: "{{ airflow_home }}/.local/bin:{{ ansible_env.PATH }}:/usr/local/bin"
  args:
    chdir: "{{ airflow_home }}"
  become_user: airflow
  when: airflow_list[0] == inventory_hostname
  tags:
    - config

- name: initialize airflow resources on kubernetes
  shell: |
    kubectl create namespace {{ airflow_kubernetes_namespace }} \
      --dry-run=client -o yaml | kubectl apply -f -

    kubectl create secret generic {{ airflow_release_name }}-airflow-s3-secret \
      --namespace {{ airflow_kubernetes_namespace }} \
      --dry-run=client \
      --from-literal=aws_access_key_id={{ airflow_s3_login }} \
      --from-literal=aws_secret_access_key={{ airflow_s3_password }} \
      -o yaml | kubectl apply -f -

    kubectl create secret generic {{ airflow_release_name }}-airflow-fernet-key \
      --namespace {{ airflow_kubernetes_namespace }} \
      --dry-run=client \
      --from-literal=fernet-key={{ airflow_fernet_key }} \
      -o yaml | kubectl apply -f -

    kubectl create secret generic {{ airflow_release_name }}-airflow-metadata \
      --namespace {{ airflow_kubernetes_namespace }} \
      --dry-run=client \
      --from-literal=connection={{ airflow_database }} \
      -o yaml | kubectl apply -f -

    kubectl create configmap {{ airflow_release_name }}-airflow-config \
      --namespace {{ airflow_kubernetes_namespace }} \
      --dry-run=client \
      --from-file={{ airflow_home }}/airflow.cfg \
      -o yaml | kubectl apply -f -
  environment:
    AIRFLOW_HOME: "{{ airflow_home }}"
    KUBECONFIG: "{{ airflow_home }}/.kube/config"
    PATH: "{{ airflow_home }}/.local/bin:{{ ansible_env.PATH }}:/usr/local/bin"
  args:
    chdir: "{{ airflow_home }}"
  become_user: airflow
  when: airflow_list[0] == inventory_hostname
  tags:
    - config

- name: create serviceaccount and config role
  shell: |
    kubectl create serviceaccount {{ airflow_release_name }}-airflow-worker-serviceaccount \
      --namespace {{ airflow_kubernetes_namespace }} \
      --dry-run=client \
      -o yaml | kubectl apply -f -

    cat << EOF | kubectl apply -f -
    ---
    kind: RoleBinding
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
      name: rolebinding-clusteradmin-{{ airflow_release_name }}-airflow-worker-serviceaccount
      namespace: {{ airflow_kubernetes_namespace }}
    roleRef:
      kind: ClusterRole
      name: admin
      apiGroup: rbac.authorization.k8s.io
    subjects:
    - kind: ServiceAccount
      name: {{ airflow_release_name }}-airflow-worker-serviceaccount
      namespace: {{ airflow_kubernetes_namespace }}
    EOF
  environment:
    AIRFLOW_HOME: "{{ airflow_home }}"
    KUBECONFIG: "{{ airflow_home }}/.kube/config"
    PATH: "{{ airflow_home }}/.local/bin:{{ ansible_env.PATH }}:/usr/local/bin"
  args:
    chdir: "{{ airflow_home }}"
  become_user: airflow
  when: airflow_list[0] == inventory_hostname
  tags:
    - config
    - service_account

- name: copy airflow pod template
  template:
    src: config/{{ airflow_version_list[:2] | join('.') }}/pod_template.yaml.j2
    dest: "{{ airflow_home }}/configs/pod_template.yaml"
    owner: airflow
    group: "{{ airflow_group }}"
    mode: 0644
  vars:
    airflow_version_list:  "{{ airflow_version.split('.') }}"
  tags:
    - config

- name: copy airflow webserver service template
  template:
    src: systemd/airflow-webserver.service.j2
    dest: /usr/lib/systemd/system/airflow-webserver.service
    mode: 0644
  tags:
    - service

- name: start airflow webserver service
  systemd:
    name: airflow-webserver
    daemon_reload: yes
    enabled: yes
    state: started
  tags:
    - service
    - start

- name: copy airflow scheduler service template
  template:
    src: systemd/airflow-scheduler.service.j2
    dest: /usr/lib/systemd/system/airflow-scheduler.service
    mode: 0644
  tags:
    - service

- name: start airflow scheduler service
  systemd:
    name: airflow-scheduler
    daemon_reload: yes
    enabled: yes
    state: started
  tags:
    - service
    - start
